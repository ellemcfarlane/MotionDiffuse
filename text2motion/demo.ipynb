{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Generation Demo\n",
    "There are three steps to generate motion sequences. \n",
    "  1. System setup\n",
    "  2. download pretrained model\n",
    "  3. generate\n",
    "\n",
    "Steps 1 and 2 are specified in [dtu_install.md](./demo/dtu_install.md). Once done with those steps, carry on hereunder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Generation\n",
    "The model generates the sequence and stores it in a `<caption>.npy` in the `args.npy_path` directory.\n",
    "\n",
    "To be able to see the sequence, the third-party `SMPLX` model needs to be run in an environment with a graphical interface (so no ssh). Detailed instructions are found below in the [SMPL-X sequence as a GIF](#replay-motion-sequence-as-smpl-x-motion-gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import utils.get_opt as opt_utils\n",
    "import utils.utils as utils \n",
    "\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "from utils.word_vectorizer import POS_enumerator\n",
    "from tools.arguments import get_case_arguments\n",
    "from tools.visualization import get_wordvec_model\n",
    "from trainers import DDPMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT THE CONDITION HERE. IF UNCONDITIONED, MAKE IT AN EMPTY STRING (i.e. \"\")\n",
    "caption = 'a person walking happily'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./checkpoints/grab/demo/opt.txt\n"
     ]
    }
   ],
   "source": [
    "SMPLX_MODEL_DIRPATH = \"./models/smplx/\"\n",
    "MODEL_DIRPATH = \"./checkpoints/grab/demo\"\n",
    "args = get_case_arguments('generation')\n",
    "\n",
    "MOTION_FRAME_N = 100\n",
    "\n",
    "# Custom definition of run arguments\n",
    "args.opt_path = pathjoin(MODEL_DIRPATH,\"opt.txt\")\n",
    "args.seed = 42\n",
    "args.model_path = MODEL_DIRPATH\n",
    "args.motion_length = MOTION_FRAME_N\n",
    "args.min_t = 0\n",
    "args.max_t = MOTION_FRAME_N\n",
    "args.npy_path = pathjoin(MODEL_DIRPATH, \"outputs\") # path to the pretrained model\n",
    "\n",
    "args.text = caption\n",
    "\n",
    "utils.set_random_seed(args.seed)\n",
    "device = utils.get_device(args)\n",
    "opt = opt_utils.get_opt(args.opt_path, device)\n",
    "\n",
    "# opt custom definitions\n",
    "opt.model_name = 'ckpt_e015' # wout .tar\n",
    "opt.do_denoise = True\n",
    "assert args.motion_length <= 196\n",
    "opt.data_root = './data/GRAB' # QUESTION (iony): Needed?\n",
    "opt.text_dir = pathjoin(opt.data_root, 'texts') # QUESTION (iony): Needed?\n",
    "opt.dim_pose = 212\n",
    "opt.max_motion_length = 196\n",
    "opt.joints_num = 22\n",
    "\n",
    "# Other configurations\n",
    "dim_word = 300\n",
    "dim_pos_ohot = len(POS_enumerator)\n",
    "\n",
    "mean = np.load(pathjoin(opt.meta_dir, 'mean.npy'))\n",
    "std = np.load(pathjoin(opt.meta_dir, 'std.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:00<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to plot airplane pass happy #100\n",
      "saving output to ./checkpoints/grab/demo/airplane_pass_happy.npy\n",
      "Motion generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Word vectorizer model\n",
    "encoder = get_wordvec_model(opt).to(device)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "trainer = DDPMTrainer(opt, encoder)\n",
    "trainer.load(pathjoin(opt.model_dir, opt.model_name + '.tar'))\n",
    "trainer.eval_mode()\n",
    "trainer.to(opt.device)\n",
    "\n",
    "result_dict = {}\n",
    "with torch.no_grad():\n",
    "    if args.motion_length != -1:\n",
    "        caption = [args.text]\n",
    "        m_lens = torch.LongTensor([args.motion_length]).to(device)\n",
    "        pred_motions = trainer.generate(caption, m_lens, opt.dim_pose)\n",
    "        motion = pred_motions[0].cpu().numpy()\n",
    "        motion = motion * std + mean # TODO: Check if this are the correct values of mean and atd\n",
    "        title = args.text + \" #%d\" % motion.shape[0]\n",
    "        print(f\"trying to plot {title}\")\n",
    "        # write motion to numpy file\n",
    "        text_no_spaces = args.text.replace(\" \", \"_\")\n",
    "        if not os.path.exists(args.npy_path):\n",
    "            os.makedirs(args.npy_path)\n",
    "        full_npy_path = f\"{args.npy_path}/{text_no_spaces}.npy\"\n",
    "        with open(full_npy_path, 'wb') as f:\n",
    "            print(f\"saving output to {full_npy_path}\")\n",
    "            np.save(f, motion)\n",
    "\n",
    "print(\"Motion generated\")\n",
    "\n",
    "\n",
    "# To retrieve the saved sequence, here u go, uncomment this\n",
    "# motion = np.load(pathjoin(args.npy_path, 'man_walking.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay motion sequence as SMPL-X motion gif\n",
    "Change directory to the `/text2motion/` and run the following command in a graphical user interface terminal (i.e. if running in DTU's HPC, run in the _thinlinc_ client).\n",
    "\n",
    "The values for the different arguments indicated in the command below are displayes in the next code cell\n",
    "\n",
    "***Note:** Remember 2 thing:\n",
    "1. To activate the conda environment mentioned in [dtu_install.md](./demo/dtu_install.md)\n",
    "2. (IF running in the hpc-thinlinc client) to run the command with vglrun in the start (`vglrun python -m ...`)\n",
    "\n",
    "```bash\n",
    "python -m datasets.motionx_explorer \\\n",
    "--model-path [model-path] \\\n",
    "--prompt [prompt]\\\n",
    "--min_t [min-t] \\\n",
    "--max_t [max-t] \\\n",
    "--display-mesh  --save-gif\n",
    "```\n",
    "\n",
    "## If that does not work...\n",
    "you can dc into the `text2motion/` and run the command `make gen`.\\\n",
    "**But** you'll need to change some parameters of the makefile to fit the promt and motion length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m datasets.motionx_explorer --model-path ./checkpoints/grab/demo --prompt airplane pass happy --min_t 0 --max_t 100 --display-mesh --save-gif\n"
     ]
    }
   ],
   "source": [
    "py_cmd = \\\n",
    "f\"python -m datasets.motionx_explorer --model-path {args.model_path} --prompt {args.text} --min_t {args.min_t} --max_t {args.max_t} --display-mesh --save-gif\"\n",
    "print(py_cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motiondiffuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
